{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student: Rodolfo Lerma\n",
    "\n",
    "# Introduction To Monte Carlo Reinforcement Learning\n",
    "\n",
    "## Machine Learning 530\n",
    "### Stephen Elsto\n",
    "\n",
    "Starting with this lesson, we will turn our attention to a **reinforcement learning**. Reinforcement learning is a distinctive type of machine learning, differing from supervised learning and unsupervised learning. \n",
    "\n",
    "Reinforcement learning has several characteristics, which differentiate this method from other machine learning and from dynamic programming. The table below outlines key differences between the model types:\n",
    "\n",
    "\n",
    "| Model Type | Environment Model | State | Labeled Data | Loss Function |\n",
    "| :--- | :---: | :---: | :---: | :----- |\n",
    "|Supervised Learning | Yes | No | Yes| Error metric |\n",
    "|Unsupervised Learning | Yes | No | No | Error metric |\n",
    "| Bandit Agent | No | No | No | Reward |\n",
    "| Dynamic Programming | Yes | Yes | No | Reward |\n",
    "| Reinforcement Learning | No | Yes | No | Reward | \n",
    "\n",
    "Some highlights of these differences include:  \n",
    "\n",
    "\n",
    "- Like dynamic programming, reinforcement learning **optimizes a reward function**. This is in contrast to supervised and unsupervised learning which attempt to minimize error or loss function.  \n",
    "- **No Markov model** needs to be specified for reinforcement learning, in contrast to dynamic programming.\n",
    "- Reinforcement learning algorithms learn by **experience**. Over time, the algorithm learns a model of the environment and these results are used to optimize the expected reward. Learning from experience is in contrast to supervised learning which uses marked cases (labels).  \n",
    "- Reinforcement learning agents take **actions** and only receive **state** and **rewards** from the environment. These are the only interaction between the RL agent and the environment.   \n",
    "\n",
    "The interaction between a reinforcement learning agent and the environment are illustrated in the figure below. Notice that the only feedback the agent receives from the environment is the reward and and state information. The agent receives no other evidence.   \n",
    "\n",
    "<img src=\"other/RL_AgentModel.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Reinforcement Learning Agent and Environment** </center>  \n",
    "\n",
    "\n",
    "**Suggested readings** for Monte Carlo reinforcement learning Chapter 5 of Sutton and Barto, second edition, provides a good introduction, including many alternative algorithms not discussed here.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Monte Carlo Reinforcement Learning\n",
    "\n",
    "A wide variety of reinforcement learning algorithms have been developed over the past few decades. In this lesson we will explore the basics of the Monte Carlo method. Monte Carlo algorithms have been known for most of the history of reinforcement learning. However, they are generally considered inefficient for several reasons that will become apparent as we proceed:\n",
    "1. Monte Carlo methods rely large numbers of **random samples** to produce estimates. Thus, Monte Carlo algorithms are inherently computationally intensive. \n",
    "2. Monte Carlo reinforcement learning algorithms must **complete an entire episode** before any reward estimate can be produce and the policy improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Monte Carlo Simulation\n",
    "\n",
    "Monte Carlo sampling was developed in the 1940s. Originally, Monte Carlo methods were used to compute estimates of complex functions which where analytically intractable. The basic idea is to **compute an estimate** of a complex function by **averaging a large number of samples**. \n",
    "\n",
    "Monte Carlo methods rely on the [**weak law of large numbers**](https://en.wikipedia.org/wiki/Law_of_large_numbers). The law of large numbers is a theorem that states that statistics of independent samples converge to the population values as more unbiased experiments are performed. We can write this mathematically for the **expected value** pr mean as:\n",
    "\n",
    "$$Let\\ \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\\\\n",
    "then\\ by\\ the\\ law\\ of\\ Large\\ Numbers\\\\\n",
    "\\bar{X} \\rightarrow E(X) = \\mu\\\\\n",
    "as\\\\\n",
    "n \\rightarrow \\infty$$\n",
    "\n",
    "Thus, if we sample some process $X$ enough times (possibly infinite), we can compute the expected value from these samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Reinforcement Learning\n",
    "\n",
    "But, how do we apply Monte Carlo sampling to reinforcement learning? More specifically, how do we apply Monte Carlo sampling to **episodic** reinforcement learning tasks. \n",
    "\n",
    "To understand this algorithm, it helps to examine the backup diagram shown below. This diagram shows Monte Carlo sampling of a single episode.    \n",
    "\n",
    "<img src=\"other/MC_Backup.JPG\" alt=\"Drawing\" style=\"width:75px; height:400px\"/>\n",
    "<center> **Backup Diagram for Monte Carlo Reinforcement Learning** </center>  \n",
    "\n",
    "Starting at the top of the diagram the system is in a state, s. An action, a, causes a transition to a new state. The sampling of the episode proceeds until the terminal state, t, is reached. The return for the initial state can only be computed once the Monte Carlo backup **ends at the terminal state**. In other words, **Monte Carlo algorithms do not bootstrap**.  \n",
    "\n",
    "In reinforcement learning we do not know the model. But, the agent can take a series of actions and find the rewards for these actions. For each episode the agent will accumulate the history of rewards for each action, given the state. \n",
    "\n",
    "Recall that for a finite or episodic Markov reward processes we define the **return** for state transitions starting with the current state. The return, or gain, is the sum of the rewards for the $T$ future states transitions of the episodic process, and can be expressed as:\n",
    "\n",
    "$$G_t = R_{t+1} + R_{t+2} + \\ldots = R_{T}= \\sum_{k = 0}^{T} R_{t+k+1}$$ \n",
    "\n",
    "Thus, for any episode the Monte Carlo algorithm will sample the return for the states visited. Over a large (actually infinite) number of episodes the Monte Carlo algorithm will sample each action value several times. The sampled return values are then averaged for each state action. This process will converge to the actual action values, which are **unobservable** directly. \n",
    "\n",
    "For sampling a single episode of a Markov process, the Monte Carlo algorithm may or may not visit a state one or more times. The question then becomes, How should returns be computed if a state is visited more than once in an episode? There are two options each of which has different statical convergence properties:\n",
    "1. **First visit** Monte Carlo estimates returns from rewards from the first visit to a state in an episode. We will use first visit Monte Carlo in this lesson.\n",
    "2. **Every visit** Monte Carlo accumulates the rewards for any visit to a state in an episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of First Visit Monte Carlo RL\n",
    "\n",
    "With this short introduction MC RL learning in mind, tet's try an example. We will sample the action value function using a simple MC algorithm here. \n",
    "\n",
    "**Navigation** to a goal is a significant problem in robotics. Real-world navigation is rather complex. Therefore, in this example we will use a simple analog called a **grid world**. The grid world for this problem is shown below. \n",
    "\n",
    "<img src=\"img/GridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **A 4x4 Grid World with Terminal State** </center>\n",
    "\n",
    "The grid world consists of a 4x4 set of positions the robot can occupy. Each position is considered a state. The goal is to navigate to state 0, the goal, in the minimum steps. We will explore methods to find policies which reach this goal and achieve maximum reward. \n",
    "\n",
    "Grid position 0 is the goal and a **terminal state**. There are no possible state transitions out of this position. The presence of a terminal state makes this an **episodic Markov random process**. For each episode sampled the robot can start in any other random position, $\\{ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \\}$. This random selection process makes this a **random start** Monte Carlo algorithm. The episode terminates when the robot enters the terminal position (state 0).  \n",
    "\n",
    "\n",
    "\n",
    "In reality, an RL agent may need to explore to find the possible actions when it is in some particular state. To simplify our example, we encode, or represent, these possibilities in a dictionary as shown in the code block below. We use a dictionary of dictionaries to perform the lookup. The keys of the outer dictionary are the identifiers (numbers) of the states. The keys of the inner dictionary are the possible actions and the values are the **successor state**, $s'$, for that transition.  \n",
    "\n",
    "In each state, there are four possible actions the robot can take:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "The MC RL agent has no model. Therefore, beyond these allowed actions, all other information is encapsulated in the environment and is unobservable by the agent. This is the key difference between reinforcement learning and dynamic programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors = {0:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':9, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':10, 'd':14, 'l':13, 'r':15},\n",
    "          15:{'u':11, 'd':15, 'l':14, 'r':15}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the environment, we need a reward structure. In this case, the robot receives the following rewards:   \n",
    "\n",
    "- 10 for entering position 0. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. \n",
    "\n",
    "This **reward structure is unknown to the MC RL agent**. The agent must **learn** the rewards by sampling the environment. Here the rewards are in the form of action values.    \n",
    "\n",
    "We encode these rewards in the same type of dictionary structure used for the foregoing structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0:{'u':10.0, 'd':10.0, 'l':10.0, 'r':10.0},\n",
    "          1:{'u':-1, 'd':-0.1, 'l':10.0, 'r':-0.1},\n",
    "          2:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          4:{'u':10.0, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          6:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          7:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          8:{'u':-0.1, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          11:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          12:{'u':-0.1, 'd':-1.0, 'l':-1.0, 'r':-0.1},\n",
    "          13:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          15:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the properties of the environment defined, it is time to create an environment simulator. The code in the cell below simulates the environment. The function is called with a state and action. It returns the next state, 's_prime' and 'reward'. \n",
    "\n",
    "To simplify the rest of the code in this notebook we are treating the dictionaries as global. *In general, this would be considered poor programming practice.* \n",
    "\n",
    "Execute the code in the cell below and observe the results from the test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, -1, False)\n",
      "(5, -0.1, False)\n",
      "(2, -0.1, False)\n",
      "(0, 10.0, True)\n"
     ]
    }
   ],
   "source": [
    "def simulate_environment(s, action, neighbors = neighbors, rewards = rewards, terminal = 0):\n",
    "    \"\"\"\n",
    "    Function simulates the environment\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward = rewards[s][action]\n",
    "    return (s_prime, reward, is_terminal(s_prime, terminal))\n",
    "\n",
    "def is_terminal(state, terminal = 0):\n",
    "    return state == terminal\n",
    "\n",
    "## Test the function\n",
    "for a in ['u', 'd', 'r', 'l']:\n",
    "    print(simulate_environment(1, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look correct. It appears the simulator works correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Policy Evaluation\n",
    "\n",
    "**Policy evaluation** is an essential part of reinforcement learning. Policy evaluation is required to compare the performance of different reinforcement learning methods. Policy evaluation is performed using state-value estimation methods. In this case, we will use a Monte Carlo state-value estimation algorithm.   \n",
    "\n",
    "To develop and test a policy evaluation algorithm, we need to define the transition probabilities for an initial policy. We set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. As there are 4 possible transitions from each state, this means all transition probabilities are 0.25. In other words, this is a random policy which does not favor any particular plan. \n",
    "\n",
    "The initial uniform transition probabilities are encoded using a dictionary of dictionaries. The organization of this data structure is identical to the foregoing data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_policy = {0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For policy evaluation We are using random start Monte Carlo. The code in the cell below generates a random state to start the Monte Carlo episode, which is not the terminal state. Execute this code and examine the results pf the test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_episode(n_states):\n",
    "    '''Function to find a random starting value for the episode\n",
    "    that is not the terminal state'''\n",
    "    state = nr.choice(range(n_states))\n",
    "    while(is_terminal(state)):\n",
    "         state = nr.choice(range(n_states))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 2, 3, 1, 11, 8, 1, 12, 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test the function to make sure never starting in terminal state\n",
    "[start_episode(15) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 8-2-1:** You will now create a function in the cell below to find an action given the state and the policy by the following steps:  \n",
    "> 1. The probability of taking an action and is determined by the transition probabilities specified by the policy. Here, you will used [numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html), the argument `p=list(policy[state].values())) + 1`, to randomly sample from the `actions` list of the `range` of possible action indicies.       \n",
    "> 2. The next state and the reward are received by making queries to the environment using the `simulate_environment` function for the current `state` and the `action` selected in the above line of code.     \n",
    "> Notice that the agent receives no specific information about the environment when selecting an action. The action is determined only by the probabilistic policy. Execute your code and examine the results of the test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d', 0, 10.0, True)\n",
      "('r', 2, -0.1, False)\n",
      "('l', 1, -0.1, False)\n",
      "('l', 2, -0.1, False)\n",
      "('u', 0, 10.0, True)\n",
      "('u', 1, -0.1, False)\n",
      "('l', 5, -0.1, False)\n",
      "('l', 6, -0.1, False)\n",
      "('r', 9, -0.1, False)\n",
      "('d', 13, -0.1, False)\n",
      "('r', 11, -0.1, False)\n",
      "('l', 10, -0.1, False)\n",
      "('d', 12, -1.0, False)\n",
      "('u', 9, -0.1, False)\n",
      "('r', 15, -0.1, False)\n",
      "('r', 15, -1.0, False)\n"
     ]
    }
   ],
   "source": [
    "def take_action(state, policy, actions = {1:'u', 2:'d', 3:'l', 4:'r'}):\n",
    "    '''Function takes action given state using the transition probabilities \n",
    "    of the policy'''\n",
    "    ## Find the action given the transistion probabilities defined by the policy.\n",
    "    ## Your two lines of code go below\n",
    "    action = actions[nr.choice(range(len(actions)), p = list(policy[state].values())) + 1]\n",
    "    s_prime, reward, is_terminal = simulate_environment(state, action)\n",
    "    \n",
    "    return (action, s_prime, reward, is_terminal)\n",
    "\n",
    "## Test function for several states\n",
    "for s in range(16):\n",
    "    print(take_action(s, initial_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results. Are the rewards given the state an action consistent for each of the three possible cases in this environment, i) transition to the terminal state, ii) transition to another non-terminal state, and iii) transition against barrier.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    "Yes, it is seem consistent with the policy and rewards. As we can see for the final state it does not matter if we have a new state it will still give a maximum reward as stated before:\n",
    "`rewards = {0:{'u':10.0, 'd':10.0, 'l':10.0, 'r':10.0}`, which is what we see in the results above.\n",
    "Also it can indentified and penalized the transition against barrier as seen in some of the cases at the edges, like: `('d', 12, -1.0, False)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We are now have all the prerequisites for the functions which do most of the work for policy evaluation. \n",
    "\n",
    "The first function is called `MC_episode`, which performs first visit Monte Carlo for one episode by the following steps:\n",
    "1. A loop is executed until episode ends when a terminal state is reached. \n",
    "2. The state transitions are determined using the policy with the `take_action` function.  \n",
    "3. Since this is first-visit Monte Carlo, the function accumulates the gain and and number of times a state has been visited. The increase in gain for previously visited states is computed as the return:   \n",
    "\n",
    "$$return = \\frac{reward-past\\ gain}{n_visits}$$\n",
    "\n",
    "The `MC_state_values` function computes the state values for a number of episodes by the following steps:\n",
    "\n",
    "1.  Iterates over the specified number of episodes. For each episode the `MC_episode` function is called and the reward accumulated. \n",
    "2. Once the episodes have concluded, the state values are normalized by dividing the accumulated rewards by the number of visits.\n",
    "\n",
    "Execute the code and examine the resulting state-values for the random walk policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def MC_episode(policy, G, n_visits, episode, n_states): \n",
    "    '''Function creates the Monte Carlo samples of one episode.\n",
    "    This function does most of the real work'''\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    states_visited = []\n",
    "    states = list(policy.keys())\n",
    "        \n",
    "    ## Find the starting state\n",
    "    current_state = start_episode(n_states)\n",
    "    terminal = False\n",
    "#    g = 0.0\n",
    "        \n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "            \n",
    "        ## Add the reward to the states visited if this is a first visit  \n",
    "        if(current_state not in states_visited):\n",
    "            ## Mark that the current state has been visited, add 1.0 to n_visits\n",
    "            ## and add reward to the gain \n",
    "            states_visited.append(current_state) \n",
    "            ## This is first vist MS, so must loop over all states and \n",
    "            ## add the reward and increment the count for the ones visited.\n",
    "            for s in range(n_states):\n",
    "                ## If state has been visited, increment n_visits and \n",
    "                ## add return; (reward - gain)/n_visits\n",
    "                if(s in states_visited):\n",
    "                    n_visits[s] = n_visits[s] + 1.0\n",
    "                    G[s] = G[s] + (reward - G[s])/n_visits[s]   \n",
    "        \n",
    "        ## Update the current state for next transition\n",
    "        current_state = s_prime \n",
    "    return (G, n_visits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_state_values(policy, n_episodes):\n",
    "    '''Function that evaluates the state value of \n",
    "    a policy using the Monte Carlo method.'''\n",
    "    ## Create list of states \n",
    "    states = list(initial_policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros((n_states))\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros((n_states))\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(n_episodes):\n",
    "        G, n_visits = MC_episode(policy, G, n_visits, i, n_states) # neighbors, i, n_states)\n",
    "    return(G) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.58241078 0.17059059 0.11852761]\n",
      " [0.57261634 0.22970139 0.12983072 0.10118236]\n",
      " [0.16294393 0.13774711 0.09451203 0.07308497]\n",
      " [0.10052253 0.08530145 0.07231508 0.06668048]]\n"
     ]
    }
   ],
   "source": [
    "## Test the functions\n",
    "nr.seed(335)\n",
    "state_values1 = MC_state_values(initial_policy, n_episodes = 10000)\n",
    "print(state_values1.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 8-2-2:** Examine the code and results above and provide short answers to these questions:   \n",
    "> 1. What happens to the state value as the number of steps required to reach the terminal state (distance on the grid) increases?   \n",
    "> 2. Why should the relationship you noted for your answer to the first question hold?   \n",
    "> 3. For first-visit Monte Carlo, why should the normalized return from a transition to a yet to be visited state in an episode be added to the states already visited in previous episodes?     \n",
    "> 4. How does the method for accumulation of normalized gain relate to the weak law of large number at the core of the Monte Carlo method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**  \n",
    "> 1. It seems that the states values decreases. Or another way to see it is that the Gain values are smaller, hinting that the model should move from that state as soon as possible to a closer state.\n",
    "> 2. If the model is looking for the best way to go to the \"0\" state, it should spend the minimum amount of time (steps) at the further away states, which is what the results are showing in the example above. \n",
    "> 3. This gives the model the ability to see what would be the best path and therefore optimize for that.       \n",
    "> 4. As for any statistics the Monte Carlo method is trying to obtain a true representation of the phenomena by means of adding more \"unbiased\" experiments, which is account by the normalization part that is making sure that \n",
    "in the long run (having perform the experiment many times) we get closer and closer to a real depiction of the phenomena/policy at hand.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "Now that we have a way to evaluate a policy using a first visit Monte Carlo algorithm we need a method to improve policy. Monte Carlo policy improvement employs the following steps:\n",
    "1. Sample rewards given state, s, and action a, following policy $\\pi$. \n",
    "2. The action values are computed from returns. \n",
    "2. The policy is updated using an $\\epsilon$-greedy algorithm. \n",
    "\n",
    "The first function, `MC_action_value_episode`, computes the action-values given the current policy for a single episode. This function uses a first visit Monte Carlo algorithm to find the action-values for a single episode with the following steps:\n",
    "\n",
    "1. The `take_action` function is used to determine the next action given the state following the policy. \n",
    "2. The visit is recoded in the `state_actions_visited` array. \n",
    "3. The core of the function is a loop over states and actions. For states that have been visited, the reward is summed and the number of visits is incremented.   \n",
    "\n",
    "The numpy array, `Q`, holds the accumulated reward. The row index is for state and the column index is for action, $\\{ up,\\ down,\\ left,\\ right \\}$. The numpy array `state_actions_visited` is indexed in the same manner.\n",
    "\n",
    "Execute this code and examine the results of the basic test cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['up', 'down', 'left', 'right'])\n",
    "    print(Q)\n",
    "\n",
    "def MC_action_value_episode(policy, Q, n_visits, inital_state, n_states, n_actions, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Function creates the Monte Carlo samples of action values for one episode.\n",
    "    This function does most of the real work'''\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    state_actions_visited = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    current_state = initial_state\n",
    "    terminal = False  \n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "\n",
    "        action_idx = action_index[action]         \n",
    "        \n",
    "        ## Check if this state-action has been visited.\n",
    "        if(state_actions_visited[current_state, action_idx] != 1.0):\n",
    "            ## Mark that the current state-action has been visited \n",
    "            state_actions_visited[current_state, action_idx] = 1.0  \n",
    "            ## This is first vist MS, so must loop over all state-action pairs and \n",
    "            ## add the reward and increment the count for the ones visited.\n",
    "            for s,a in list(itertools.product(range(n_states), range(n_actions))):\n",
    "                ## Add reward to if these has been a visit to the state\n",
    "                if(state_actions_visited[s,a] == 1.0):\n",
    "                    n_visits[s,a] = n_visits[s,a] + 1.0\n",
    "                    Q[s,a] = Q[s,a] + (reward - Q[s,a])/n_visits[s,a]    \n",
    "        ## Update the current state for next transition\n",
    "        current_state = s_prime\n",
    "    return (Q, n_visits) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      "           up      down      left     right\n",
      "0    0.000000  0.000000  0.000000  0.000000\n",
      "1    0.000000  0.000000  0.000000  0.000000\n",
      "2    0.000000  0.000000  0.000000  0.000000\n",
      "3    0.000000  0.000000  0.000000  0.000000\n",
      "4   10.000000  0.000000  0.000000  0.000000\n",
      "5    0.000000  0.000000  4.950000  0.000000\n",
      "6    0.000000  0.000000  3.266667  0.000000\n",
      "7    0.000000  0.000000  0.000000  0.000000\n",
      "8    0.000000  0.000000  0.000000  0.000000\n",
      "9    0.000000  0.000000  0.000000  0.000000\n",
      "10   2.425000  1.920000  0.000000  0.000000\n",
      "11   0.000000  0.000000  0.000000  0.000000\n",
      "12   0.000000  0.000000  0.000000  0.000000\n",
      "13   0.000000  0.000000  0.000000  0.000000\n",
      "14   1.583333  1.214286  0.000000  0.000000\n",
      "15   0.000000  0.640000  1.050000  0.822222\n",
      "\n",
      "n_visits\n",
      "     up  down  left  right\n",
      "0   0.0   0.0   0.0    0.0\n",
      "1   0.0   0.0   0.0    0.0\n",
      "2   0.0   0.0   0.0    0.0\n",
      "3   0.0   0.0   0.0    0.0\n",
      "4   1.0   0.0   0.0    0.0\n",
      "5   0.0   0.0   2.0    0.0\n",
      "6   0.0   0.0   3.0    0.0\n",
      "7   0.0   0.0   0.0    0.0\n",
      "8   0.0   0.0   0.0    0.0\n",
      "9   0.0   0.0   0.0    0.0\n",
      "10  4.0   5.0   0.0    0.0\n",
      "11  0.0   0.0   0.0    0.0\n",
      "12  0.0   0.0   0.0    0.0\n",
      "13  0.0   0.0   0.0    0.0\n",
      "14  6.0   7.0   0.0    0.0\n",
      "15  0.0  10.0   8.0    9.0\n"
     ]
    }
   ],
   "source": [
    "## Basic test of the function\n",
    "n_actions = 4\n",
    "n_states = 16\n",
    "initial_state = 15\n",
    "\n",
    "#Containers\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "n_visits = np.zeros((n_states, n_actions))\n",
    "\n",
    "Q, n_visits = MC_action_value_episode(initial_policy, Q, n_visits, initial_state, n_states, n_actions)\n",
    "\n",
    "#Print results\n",
    "print('Q')\n",
    "print_Q(Q)\n",
    "print('\\nn_visits')\n",
    "print_Q(n_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 8-2-3:** Provide short answers to the following questions: \n",
    "> 1. Why are the action values for the terminal state 0.0?\n",
    "> 2. After the single episode test, explain why some state action-values have not been visited. \n",
    "> 3. Provide a short explanation of the difference between the action values shown above and the state values you examined for Exercise 8-2-2.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1. Because once you get to the terminal state the model should not be looking ahead for the next best step. Or in other words once you get to the terminal state we should not see any action (up/down/left/right) to another state.     \n",
    "> 2. For this particular example (starting at state 15 - the farthest away from the terminal state). We can see that the Monte Carlo simulation for decided for one path 15(left) -> 14(up) -> 10(up) -> 6(left) -> 5(left) -> 4(up) -> 0, this given the `action-values` connection or in otherwords given that the gains depend on the action and state and not only the state as previously computed. Also this is just one episode, it is likely that once we explore this problem thru multiple episodes more states will be shown.     \n",
    "> 3. In Excercise 8-2-2 the gain is connected to the state only, while in this excercise the gain is linked to the state and the action which provides the model a better understanding of a `action-values` and therefore a better way to explore the grid. Also in Exercise 8-2-2 we explore 10,000 episodes while in this Exercise we have only explored one.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in the cell below computes the action values using the specified number of episodes, using first visit Monte Carlo. The steps are:\n",
    "\n",
    "1. For each episode, the `MC_action_value_episode` function is called. \n",
    "2. The accumulated rewards are normalized by the number of visits to compute the action values. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_action_values(policy, Q, n_episodes, inital_state):\n",
    "    '''Function evaluates the action-values given a policy for the specified number of episodes and \n",
    "    initial state'''\n",
    "    n_states = len(policy)\n",
    "    n_actions = len(policy[0])\n",
    "    ## Array to count visits to action-value pairs\n",
    "    n_visits = np.zeros((n_states, n_actions))\n",
    "    ## Dictionary to hold neighbor states\n",
    "    neighbors = {}\n",
    "    \n",
    "    ## Loop over number of episodes\n",
    "    for _ in range(n_episodes):\n",
    "        ## One episode of MC\n",
    "        Q, n_visits = MC_action_value_episode(policy, Q, n_visits, initial_state, n_states, n_actions)\n",
    "    return(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           up      down       left     right\n",
      "0    0.000000  0.000000   0.000000  0.000000\n",
      "1    0.694688  0.488149  10.000000  0.413719\n",
      "2    0.382789  0.345640   0.669286  0.278522\n",
      "3    0.286880  0.267063   0.370841  0.287342\n",
      "4   10.000000  0.403342   0.805459  0.478892\n",
      "5    0.629159  0.336362   0.706739  0.318910\n",
      "6    0.323328  0.244879   0.389152  0.260047\n",
      "7    0.243235  0.223252   0.277582  0.217181\n",
      "8    0.685875  0.291068   0.368290  0.353854\n",
      "9    0.430487  0.248030   0.344698  0.248344\n",
      "10   0.260914  0.182897   0.247696  0.190227\n",
      "11   0.177240  0.188981   0.195851  0.159904\n",
      "12   0.400146  0.268503   0.291949  0.249553\n",
      "13   0.273659  0.243350   0.234258  0.216990\n",
      "14   0.196513  0.161939   0.186566  0.165380\n",
      "15   0.127283  0.098541   0.124474  0.102906\n"
     ]
    }
   ],
   "source": [
    "## Basic test of the function\n",
    "n_episodes = 1000\n",
    "initial_state = 15\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "#Updated policy\n",
    "Q = MC_action_values(initial_policy, Q, n_episodes, initial_state)\n",
    "print_Q(Q)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 8-2-4:** Examine these action values an provide short answers to the following questions.   \n",
    "> 1. Which action values correspond to transition to the terminal state and are these values what you expect given the environment?   \n",
    "> 2. Why are the action values for the lower right state reasonable given the environment model?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1. Yes, it is possible to see that `4-UP` & `1-LEFT` the action values for this transitions to the terminal state are 10. This is a little bit surprising based on the equation used to calculate this value: Q[s,a] = Q[s,a] + (reward - Q[s,a])/n_visits[s,a]. The accumulated reward (Q) depends on the the previous accumulated reward plus the average of the difference between the reward at that state and the previous accumulated reward, and based on the value it seems that the difference between the positive and negative reward is big enough to drive the final accomulated reward to 10.     \n",
    "> 2. Similar reason as before, the accumualted reward function depend on the previous accomulated reward and the reward at that current action-state position and since the model is penalized to state on those far away positions (by having a negative reward) is expected to see this lower values at the lower right corner. Also since the initial state for the model is the far right lower corner, it is likely that the model states around this location for some time while is learning to move its way to the top which will lead to a smaller number as the Q value depends inversely to the number of times the model visit that particular value.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the code required to perform first visit Monte Carlo action value estimation, its time to perform policy improvement. \n",
    "\n",
    "The `update_policy` function in the cell below performs $\\epsilon$-greedy policy improvement given the action values. The steps are:\n",
    "\n",
    "1. Loop over all states to update the policy for each state. \n",
    "2. For each state find the actions with the maximum action value. The best action may not be unique. \n",
    "3. The transition probability is computed for the actions with the largest action value. All other actions will be assigned a transition probability of $\\epsilon$.\n",
    "4. The policy is updated with the new action values. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    print(pd.DataFrame.from_dict(policy, orient='index'))\n",
    "\n",
    "def update_policy(policy, Q, epsilon, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Updates the policy based on estiamtes of Q using \n",
    "    an epslion greedy algorithm. The action with the highest\n",
    "    action value is used.'''\n",
    "    \n",
    "    ## Find the keys for the actions in the policy\n",
    "    keys = list(policy[0].keys())\n",
    "    \n",
    "    ## Iterate over the states and find the maximm action value.\n",
    "    for state in range(len(policy)):\n",
    "        ## First find the index of the max Q values  \n",
    "        q = Q[state,:]\n",
    "        max_action_index = np.where(q == max(q))[0]\n",
    "\n",
    "        ## Find the probabilities for the transitions\n",
    "        n_transitions = float(len(q))\n",
    "        n_max_transitions = float(len(max_action_index))\n",
    "        p_max_transitions = (1.0 - epsilon *(n_transitions - n_max_transitions))/(n_max_transitions)\n",
    "  \n",
    "        ## Now assign the probabilities to the policy as epsilon greedy.\n",
    "        for key in keys:\n",
    "            if(action_index[key] in max_action_index): policy[state][key] = p_max_transitions\n",
    "            else: policy[state][key] = epsilon\n",
    "    return(policy)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       u     d     l     r\n",
      "0   0.25  0.25  0.25  0.25\n",
      "1   0.10  0.10  0.70  0.10\n",
      "2   0.10  0.10  0.70  0.10\n",
      "3   0.10  0.10  0.70  0.10\n",
      "4   0.70  0.10  0.10  0.10\n",
      "5   0.10  0.10  0.70  0.10\n",
      "6   0.10  0.10  0.70  0.10\n",
      "7   0.10  0.10  0.70  0.10\n",
      "8   0.70  0.10  0.10  0.10\n",
      "9   0.70  0.10  0.10  0.10\n",
      "10  0.70  0.10  0.10  0.10\n",
      "11  0.10  0.10  0.70  0.10\n",
      "12  0.70  0.10  0.10  0.10\n",
      "13  0.70  0.10  0.10  0.10\n",
      "14  0.70  0.10  0.10  0.10\n",
      "15  0.70  0.10  0.10  0.10\n"
     ]
    }
   ],
   "source": [
    "policy = update_policy(initial_policy, Q, 0.1)    \n",
    "print_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 8-2-5:** Examine the improved policy. Given the possible paths toward the goal on the grid world and the value of $\\epsilon$ do these transition probabilities seem reasonable.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**\n",
    "It does as the model has to decide for a particular path even though there are many options that would lead to similar performance (many optimal paths for this particular problem). In this case the model is optimizing the following way:\n",
    "\n",
    "- If you are in the lowest row go up (15-14-13-12 UP)\n",
    "- 11 LEFT\n",
    "- 10,9,8 UP\n",
    "- 7,6,5 LEFT\n",
    "- 4 UP\n",
    "- 3,2,1 LEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 8-2-6:** It is now time to evaluate the improved policy and compare it to the initial policy you examined in Exercise 8-2-2. In the cell below provide the code to perform Monte Carlo policy evaluation for 10,000 episodes for the improved policy. Then, execute your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "def MC_policy_improvement(policy, n_episodes, n_cycles, inital_state = 15, epsilon = 0.1, n_actions = 4):\n",
    "    Q = np.zeros((len(policy), n_actions))\n",
    "    for _ in range(n_cycles):\n",
    "        Q = MC_action_values(policy, Q, n_episodes, inital_state)\n",
    "        policy = update_policy(policy, Q, epsilon = epsilon)\n",
    "    return(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_policy = MC_policy_improvement(initial_policy, 5000, 5, epsilon = 0.1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         5.69300267 2.26373016 1.72559679]\n",
      " [5.44408767 2.78192279 1.9237238  1.51057441]\n",
      " [2.49852293 1.89600815 1.42525283 1.15951505]\n",
      " [1.78156288 1.40213819 1.13231026 0.90779068]]\n"
     ]
    }
   ],
   "source": [
    "nr.seed(369)\n",
    "state_values = MC_state_values(improved_policy, n_episodes = 10000)\n",
    "#Improved State\n",
    "print(state_values.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.58241078 0.17059059 0.11852761]\n",
      " [0.57261634 0.22970139 0.12983072 0.10118236]\n",
      " [0.16294393 0.13774711 0.09451203 0.07308497]\n",
      " [0.10052253 0.08530145 0.07231508 0.06668048]]\n"
     ]
    }
   ],
   "source": [
    "#State One\n",
    "print(state_values1.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the two sets of state values, briefly describing the evidence you see of improvement in the policy from the initial uniform policy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 8-2-7:** What happens if the value of $\\epsilon$ is reduced? To find out perform policy Monte Carlo policy improvement on the `initial_policy` using $\\epsilon = 0.01$ and print the improved policy. Next, perform Monte Carlo policy evaluation for 10,000 episodes for the improved policy. Then, execute your code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n",
    "improved_policy_2 = MC_policy_improvement(initial_policy, 100, 5000, epsilon = 0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(369)\n",
    "state_values3 = MC_state_values(improved_policy_2, n_episodes = 10000)\n",
    "print(state_values3.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the policy and state values you have just computed to the results using $\\epsilon=0.1$. Is this new policy better and why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  \n",
    "The improved policy makes sense. Transitions that move the robot closer to the goal are favored. \n",
    "Finally, execute the code in the cell below to compute the returns for the improved policy. \n",
    "These returns are significantly higher than for the random policy, indicating the policy is indeed an improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, 2019, 2022, Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
